# ğŸ§  Reliable Evaluation & Drift Detection in Large Language Models (LLMs)
https://doi.org/10.5281/zenodo.16615347

> A research-grade project showcasing methods for hallucination detection, prompt drift scoring, and GPU-efficient inference for production-grade LLMs.

![drift](./figures/drift_comparison_chart.png)

## ğŸ“Œ Overview

This project simulates a real-world ML/AI research workflow for **reliable LLM evaluation** across model versions and prompt variations. It includes:

- Hallucination detection and factuality scoring  
- Prompt drift detection using tone, length, and instruction adherence metrics  
- Latency and cost benchmarking across GPT-4, Claude 3, and Mixtral  
- Streamlit dashboards for visual comparison and exploratory evaluation  
- Simulated experimental dataset with realistic LLM outputs and scoring  

ğŸ§ª Designed to serve as a **proof-of-concept** for AI safety, GenAI product evaluation, and R&D workflows in enterprise settings.

---

## ğŸ“ Project Structure

PhD_Thesis_Repo/
â”œâ”€â”€ data/ # Simulated LLM outputs and scoring metadata
â”‚ â”œâ”€â”€ hallucination_gpt4.csv
â”‚ â””â”€â”€ drift_samples_claude.csv
â”œâ”€â”€ figures/ # Diagrams for latency, hallucination, drift metrics
â”‚ â”œâ”€â”€ hallucination_histogram.png
â”‚ â””â”€â”€ drift_comparison_chart.png
â”œâ”€â”€ src/ # Core Python logic
â”‚ â”œâ”€â”€ hallucination_detector.py
â”‚ â””â”€â”€ drift_scoring.py
â”œâ”€â”€ notebooks/ # Jupyter notebooks for data exploration
â”‚ â”œâ”€â”€ 01_data_eda.ipynb
â”‚ â””â”€â”€ 02_llm_eval_comparison.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

yaml
Copy
Edit

---

## ğŸ”¬ Features

- âœ… **Hallucination Detection** using keyword-based scoring and GPT-graded evaluation  
- âœ… **Prompt Drift Detection** with rubric-based scoring on tone, length, verbosity  
- âœ… **Streamlit UI** for visual exploration of model behavior  
- âœ… **Metrics Dashboard** for latency, output quality, hallucination rate  
- âœ… **Energy/Inference Tradeoff** insights via simulated benchmarks  
- âœ… Designed for **enterprise LLM evaluation pipelines**  

---

## ğŸ“Š Example Outputs

| Model     | Hallucination Rate | Latency (ms) | Prompt Drift Score |
|-----------|--------------------|--------------|---------------------|
| GPT-4     | 8.5%               | 1025         | 0.32                |
| Claude 3  | 7.2%               | 875          | 0.41                |
| Mixtral   | 12.1%              | 495          | 0.62                |

---

## ğŸ›  Tech Stack

- Python Â· pandas Â· scikit-learn  
- Streamlit Â· matplotlib Â· seaborn  
- Simulated data with GPT-4/Claude-style outputs  
- Jupyter/Colab Notebooks for EDA  
- GitHub Actions ready for CI  

---

## âœï¸ Author

**Eva Paunova**  
PhD Researcher (in progress) in AI Evaluation  
GitHub: [@epaunova](https://github.com/epaunova)  
Medium: [@evapaunova](https://medium.com/@evapaunova)  

---
